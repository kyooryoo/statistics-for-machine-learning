# 机器学习中的概率统计
学习概率统计的时候结合具体场景也许有更深入理解。  
这里结合机器学习和Python语言，温习概率统计知识。  

### 条件概率
研究独立事件概率的意义不大，实际生活中也很少独立事件。  
给定某事件发生概率，在此基础上研究另一事件的发生概率。  

可以理解为两个事件发生概率的交集，即同时发生的概率：  
A事件发生的概率乘以A事件发生前提下B事件的发生概率。  
数学表达为P(AB)=P(B|A)P(A)或P(B|A)=P(AB)/P(A)  

### 独立事件
一般情况下某个事件的独立概率值和条件概率值不同。  
但如果相同就意味着两事件之间没有影响，即相独立。  
即P(AB)=P(A)P(B)，一般只有时空无交集才能独立。  

### 全概率公式
全事件的子事件，每次必发生其一且任意两个不会同时发生。  
引入额外事件，将其概率按分配到全事件的所有子事件中去。  
其概率为各子事件条件概率以子事件概率为权重的加权平均。  
其值在子事件与额外事件片的条件概率最小值和最大值之间。  
P(A)=P(B1)P(A|B1)+P(B2)P(A|B2)+...+P(Bn)P(A|Bn)

### 贝叶斯公式
P(Bi|A)=P(ABi)/P(A)=P(Bi)P(A|Bi)/P(A)=  
P(Bi)P(A|Bi)/(P(B1)P(A|B1)+...+P(Bn)P(A|Bn))  
注意它通过如下等式将P(B|A)和P(A|B)联系到了一起：  
* P(AB)=P(B)P(A|B)
* P(A)=(P(B1)P(A|B1)+...+P(Bn)P(A|Bn))

贝叶斯公式的意义在于推断造成A结果的各种B原因的概率。  
作为原因的B可能不止一个，所以一般用P(Bi|A)来表示。  
在由果追因中贝叶斯公式会很有用，基础理论是条件概率。  

### 独立性
独立事件即P(A|B)=P(A)也就是B对A的发生概率没有影响。  
再由P(A|B)的定义，也就是P(AB)/P(B)可以得到：  
P(A|B)=P(A)=P(AB)/P(B) -> P(AB)=P(A)P(B)    

### 不相容
如果AB事件不相容，意味着他们的交集为空，有P(AB)=0  
由于P(A)和P(B)都不为0，所以P(A)P(B)!=0=P(AB)  
因此，不相容的事件AB一定不是独立的事件。    
逻辑上，不相容事件AB中一个发生意味着另一个不发生。  
所以不相容事件相互引入了额外信息，因而不相互独立。  

### 条件独立
前面已经提到，独立事件满足P(AB)=P(A)P(B)  
在以上加入条件C，即P(AB|C)=P(A|C)P(B|C)  

先看P(AB|C)=P(ABC)/P(C)=P(A|BC)P(BC)/P(C)  
=P(A|BC)P(B|C)P(C)/P(C)=P(A|BC)P(B|C)  

再根据条件独立的定义，即P(AB|C)=P(A|C)P(B|C)  
得到P(A|BC)P(B|C)=P(A|C)P(B|C)即P(A|BC)=P(A|C)  
即事件C发生的前提下，事件B是否发生并不影响事件A的发生。  

### 独立与条件独立
假设A事件是第一次抛硬币得到硬币正面向上的结果。  
假设B事件是第二次抛硬币得到硬币正面向上的结果。  
假设C条件是两次抛硬币的结果不可以相同。  
显然AB事件是互不影响的独立事件，满足P(AB)=P(A)P(B)  

但根据以上事件和条件定义，有P(AB|C)=0  
因为AB事件有相同结果，C条件规定两个事件结果不同。  
同时有P(A|C)!=0且P(B|C)!=0
所以有P(AB|C)=0!=P(A|C)P(B|C)!=0

以上结论是，独立与条件独立并没有必然联系。  

### 伯努利实验
实验由一系列小实验组成，且每个小实验只有两种可能结果。  
伯努利实验是一种独立重复实验，如连续抛硬币的实验就是。  

## 离散型随机变量
关注变量的取值，取值对应的概率，和统计特征与度量方法。  

### 分布列
分布列描述离散随机变量的每种取值及其对应的概率。  
还以抛硬币为例，设抛n次，每次结果硬币正面向上概率为p。  
当n足够大时，结果向上的次数和概率的对应关系如下：  
取值 ｜ 0 ｜ 1 ｜ 2 ｜ 其他
p   ｜1/4｜1/2｜1/4｜  0

### 概率质量函数
即PMF，将随机变量的值映射到其概率上，用图形表达其分布。  

### 二项分布
随机变量只有可能取两个值的时候，如抛硬币得到的正反结果。  
连续抛n次，得到对应随机变量值的概率，参考0201的分布图。  
解读方式为：当每抛一次得到正面的概率p为0.25时，抛10次  
得到结果为2的概率最高，大约为0.25，也就是25%左右。  

这里有个发现，即p=0.5时抛10次得到5次正面的概率仅为0.25。  
虽然每抛都有50%可能正面，但抛10次得到5正面的概率不是50%。  
实际上，得不到5次正面和得到5次正面的赔率是3:1，是蛮大的。  
另外要注意的是，得不到5次正面和得到5次反面也不是一回事。  

实际上不论p值如何，得到任意次正面的最大概率始终是0.25左右。  

### 几何分布
几何分布考量的是，连续抛掷硬币到相应次数正面结果的概率。  
如当p为0.5时，抛掷一次得正面的概率就是0.5，二次为0.25。  
而当p为0.25时，抛掷一次得正面的概率就是0.25，类推。  
概率质量函数的图形表达参考0202的Python脚本绘图结果。  

### 泊松分布
n次独立伯努利实验成功的概率为p，是服从二项分布的随机变量。  
当n非常大近无穷而p非常小近零时，泊松分布与二项分布近似。  
泊松分布函数只需要一个参数λ=n*p，查看0206生成的图形。  
可以从图形观察到，随着λ的增大，最大概率值呈下降趋势。  

## 连续随机变量
汽车速度和设备运行时间等，取值于连续区域的随机变量也很普遍。  
离散随机变量有PMF概率质量函数，连续随机变量有概率密度函数PDF。  
单个点的概率密度函数取值不是概率，而是概率律，因此可以大于1。  
一般讨论连续随机变量在一个区域内的取值概率，而不是单点的概率值。  

随机变量的区间概率通过积分计算，概率密度函数有非负性和归一性：  
非负性即对一切x都有概率值大于0，归一性即全部概率值总和为1。  

### 正态分布
正态分布是一种连续随机变量概率分布，在自然和社会现象中普遍存在。  
函数带有两个参数均值和标准差，均值决定位置，标准差决定形态。  
使用norm函数代入loc和scale分别代表均值和标准差，参考0208。  

### 指数分布
一般用来表现到某个事件发生所用的时间，比如一台设备的残值。  
其图形特征是当随机变量超过某个值时概率随变量值增加呈指数递减。  
指数函数只需要一个参数Scale，参考0210和0211的图形和采样。  

### 均匀分布
随机变量在区间loc到loc+scale之间拥有一个恒定值，处处相等。  
参考0212和0213查看均匀分布的函数图形和采样。  

## 多元随机变量
同一个实验结果可能产生多个随机变量，其取值存在相互关联。  
以下从离散型随机变量开始考察多远随机变量的分布。  

### 联合分布
任意一组随机变量都有对应的联合概率值，若干组集合也是同理。  
如果联合分布列包含XY两个随机变量，则任意XiYi有对应的概率值。  
如果两个事件可以映射到XiYi和XjYj那么总概率就是概率值之和。  
最后，如果将联合分布列中的概率汇总结果必然是1，满足归一性。  

### 边缘分布
边缘分布可以看作是联合分布的降维，如XiY1+XiY2+...+XiYn  
如上处理得到的概率即X维度上的边缘概率：X1, X2, ..., Xn  
可以理解为边缘概率只与指定维度的变量，如X自己有关，与Y无关。  
相应的，联合概率的每个概率值由每个维度上的随机概率共同决定。  

### 条件分布
所谓条件可以是已知某个事件的发生或已知某个条件概率的确定值。  
所谓条件分布就是在如上已知条件下随机变量的概率的分布情况。  
例如看Y=y2条件下随机变量的条件分布，首先计算边缘概率P(y2)  
接着计算每个P(xi,y2)/P(y2)的条件概率，结果应该满足归一性。  

### 期望和方差
如果随机变量相互独立，那么他们乘积的期望等于各自期望的乘积。  
同时，独立随机变量各自边缘分布列的乘积等于其联合分布列的值。  
再有，独立随机变量的和的方差等于各变量各自方差的和。  
随机变量的和的期望等于各变量各自期望的和，它不需要独立条件。  

### 量化相关性
如果相关，我们会想了解一个变量变化时另一个变量发生多大变化。  
定量指标是协方差，即各变量偏离各自预期的差，的乘积，的预期。  

